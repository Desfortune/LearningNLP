{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningNLP_Tutorial1.1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hISWdom_eFdQ"
      },
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MachineLearningJournalClub/LearningNLP/blob/main/LearningNLP_Tutorial1.ipynb)\n",
        "\n",
        "# Learning NLP Tutorial Series \n",
        "## Tutorial 1 : More Sentiment Analysis \n",
        "\n",
        "Topics include: \n",
        "* Exploring a dataset (Disaster Tweets, ArXiv) \n",
        "* Explainability methods : SHAP, LIME \n",
        "* Sentiment Analysis generalization to N classes \n",
        "\n",
        "(Authors: Luca Bottero, Simone Azeglio)\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "* [Preprocessing](#section1)\n",
        "    * [Feature Engineering](#section1.1)\n",
        "    \n",
        "\n",
        "* [Explainability Methods](#section2)\n",
        "    * [SHAP](#section2.1)\n",
        "    * [LIME](#section2.2)\n",
        "\n",
        "* [References & Additional Material](#section4)\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "<a id='section1'></a>\n",
        "# **Preprocessing**\n",
        "In this first part ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsrMEupxz9-P",
        "outputId": "388b20dc-473f-4323-fc84-2caaba00ca3a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFAh_4eRun7A"
      },
      "source": [
        "The Arxiv dataset we will use is written in JSON, a syntax for storing and exchanging data.\n",
        "JSON is text, written with JavaScript object notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce2vZGH9urYi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json   #importing this module we can work with JSON data\n",
        "import nltk   #NLP toolkit\n",
        "from nltk.corpus import stopwords\n",
        "import re     # library for regular expression operations\n",
        "import string # for string operations\n",
        "import collections\n",
        "import gensim  \n",
        "from gensim import parsing        # Help in preprocessing the data, very efficiently\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLT2lAXDxtH8"
      },
      "source": [
        "With the function in the next cell we build an object called generator, i.e. a kind of iterable you can only iterate over once.\n",
        "A generator don't store all the values in memory.\n",
        "So, with the function get_metadata() you can open the file in order to manage it paper by paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4S3YATuuspm"
      },
      "source": [
        "def get_metadata():\n",
        "    with open('/content/gdrive/MyDrive/ColabNotebooks/arxiv-metadata-oai-snapshot.json') as f:\n",
        "        for line in f:\n",
        "            yield line #Yield is used like Return, except the function will return a generator"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxEbL_rWuss2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "81f72955-0858-47f2-88ba-f8f6ff328983"
      },
      "source": [
        "metadata = get_metadata()\n",
        "\n",
        "for paper in metadata:\n",
        "    first_paper = json.loads(paper) #json.loads() return a dictionary\n",
        "    break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-52ba8d0d88dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfirst_paper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#json.loads() return a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-5d5877c02b18>\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/ColabNotebooks/arxiv-metadata-oai-snapshot.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m \u001b[0;31m#Yield is used like Return, except the function will return a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ColabNotebooks/arxiv-metadata-oai-snapshot.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwL6aqgYuz4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "221eaf38-6d6b-4d5e-e30c-a87e945c73b2"
      },
      "source": [
        "for key in first_paper:\n",
        "    print(key)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3273219af33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_paper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'first_paper' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHUd8_HMu3V5"
      },
      "source": [
        "We're interested only in the keys Categories, Authors, Title and Abstract of each paper, so let's save this information in a Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykryHbj_u4yt"
      },
      "source": [
        "#set of empty list that will be filled with the information of each paper\n",
        "\n",
        "categories = []\n",
        "authors = []\n",
        "title = []\n",
        "abstract = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RQSbi1ou56d"
      },
      "source": [
        "total_items = 0\n",
        "\n",
        "for papers in metadata:\n",
        "    paper = json.loads(papers)\n",
        "    \n",
        "    categories.append(paper['categories'])\n",
        "    authors.append(paper['authors'])\n",
        "    title.append(paper['title'])\n",
        "    abstract.append(paper['abstract'])\n",
        "    \n",
        "    total_items += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR58x4Z9u597"
      },
      "source": [
        "print(total_items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2mdHTIxvDOJ"
      },
      "source": [
        "#In this cell we create a dictionary with the information stored before\n",
        "d = {\n",
        "    'Categories': categories,\n",
        "    'Authors': authors,\n",
        "    'Title': title,\n",
        "    'Abstract': abstract,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Gq36AZvDRY"
      },
      "source": [
        "df = pd.DataFrame(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJCddMyPvFVJ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-grrP_XcvJEV"
      },
      "source": [
        "In order to use this data for classification we have to prepocessing them, so we exploit Gensim library (reference at the following link https://radimrehurek.com/gensim/corpora/textcorpus.html).\n",
        "The following code has been ispired from the following notebook found on kaggle: https://www.kaggle.com/anurag3753/prediction-naive-bayes-preprocessing-with-gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8azoNkJvJKG"
      },
      "source": [
        "def transformText(text):\n",
        "    \n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    # Convert text to lower\n",
        "    text = text.lower()\n",
        "    # Removing non ASCII chars    \n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
        "    \n",
        "    # Strip multiple whitespaces\n",
        "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
        "    \n",
        "    # Removing all the stopwords\n",
        "    filtered_words = [word for word in text.split() if word not in stops]\n",
        "    \n",
        "    # Removing all the tokens with lesser than 3 characters\n",
        "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
        "    \n",
        "    # Preprocessed text after stop words removal\n",
        "    text = \" \".join(filtered_words)\n",
        "    \n",
        "    # Remove the punctuation\n",
        "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
        "    \n",
        "    # Strip all the numerics\n",
        "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
        "    \n",
        "    # Strip multiple whitespaces\n",
        "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
        "    \n",
        "    # Stemming\n",
        "    return gensim.parsing.preprocessing.stem_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97xaIwryvMs7"
      },
      "source": [
        "df['Title'] = df['Title'].map(transformText)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyQjjybpvM7Z"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB_lxf8ZvTg7"
      },
      "source": [
        "First we have to chose only two Categories in order to perform our binary classification. At the page https://arxiv.org/category_taxonomy you can find the complete ArXiv categories taxonomy. So, for our purpose we chose the two more frequent categories. Let's find them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4RjZTpmvT2u"
      },
      "source": [
        "categories = df.Categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w0BuxTevYpB"
      },
      "source": [
        "cat_freq_dic = collections.Counter(categories) #collections.Counter gives us a dictionary with a count of how many \n",
        "                                               #times a category appears in the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kQwmpm1vYxc"
      },
      "source": [
        "max1 = 0\n",
        "max2 = 0\n",
        "for key in cat_freq_dic:\n",
        "    if  cat_freq_dic[key]>max1:\n",
        "        max1=cat_freq_dic[key]\n",
        "        max1key=key\n",
        "    elif cat_freq_dic[key]>max2:\n",
        "        max2=cat_freq_dic[key]\n",
        "        max2key=key        \n",
        "            \n",
        "print(max1key, max1)\n",
        "print(max2key, max2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Rx7wBXvclf"
      },
      "source": [
        "traindf = df[(df['Categories']==max2key) | (df['Categories']==max1key)]\n",
        "traindf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMy7tKYMvi1L"
      },
      "source": [
        "Now that only two categories has been selected, we have to convert categories names in a digits in order to be processed by a classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYgbJMNSvcrW"
      },
      "source": [
        "category_to_id = {    #create a simple dictionary which map the category in a digit\n",
        "    max1key: 0,\n",
        "    max2key: 1\n",
        "}\n",
        "\n",
        "def get_category_id(category):\n",
        "    return category_to_id[category]\n",
        "\n",
        "traindf['Categories'] = traindf['Categories'].map(get_category_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1nre0rvlny"
      },
      "source": [
        "traindf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihuK2Zu5vp9b"
      },
      "source": [
        "Once we have properly preprocess our data, we have to split the dataset in training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aafMIAzVvlqn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(traindf['Title'], traindf['Categories'], \n",
        "                                                    test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqEku8ZWvlu6"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR9B3KzKv6e9"
      },
      "source": [
        "# **Feature extraction with count vectorizer and term frequency-inverse document frequency (tfidf)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xN5bKi-v6hd"
      },
      "source": [
        "Now we have to create the features will feed our classification model. In order to do that we exploit to methods: CountVectorizer and TfidfTransofer. \n",
        "\n",
        "CountVectorizer (reference at the following link https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is able to create a dictionary of word inside all the documents we provide to it and than to represent each of this documents (the titles) in a matrix form. Every row will be a title and every column a word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4fKKCpMvlxt"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMbCePakwCL6"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "features_name = vectorizer.get_feature_names()\n",
        "features_name[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1veHF3fwGhh"
      },
      "source": [
        "Because of the high number of word in the vocabulary, the resulting matrix after applying CountVectorizer to our data is a sparse matrix, with most of its values equal to zero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRXEwqNuwCO7"
      },
      "source": [
        "len(features_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iKsTXcywJx3"
      },
      "source": [
        "print(X_train_counts.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di9mhLlwwMAD"
      },
      "source": [
        "After the data manipulation above, we have to use the TfidfTransformer (reference at the following link https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) in order to create a proper count of the frequency of each word inside our dataset. Tf-idf is the acronym for Term Frequncy-Inverse Document Frequency. With this approach mw evaluate the relative importance of particular word. Tf-idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the Term Frequency is the \"raw frequency\" of a term in a document, i.e. the number of times a term occurs in document (a title). The \"inverse document frequency\" is a measure of whether the term is common or rare across all documents. It is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. The Tf-idf is the product of this two quantity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlV7dLFVwMOX"
      },
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y4caJh0wR76"
      },
      "source": [
        "# **Training a classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cHOcGOdwSvE"
      },
      "source": [
        "We choose Logistic Regression as classification model. Instead of making a manual implemetation of this model, we exploit the sklearn method for Logistic Regression (reference at the following link https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCG6dgL8wS2s"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "regression = LogisticRegression()\n",
        "regression.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JApBnlNowgfh"
      },
      "source": [
        "# **Prediction with test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQw8O3afwgiG"
      },
      "source": [
        "Pay attention to the methods used to countvectorize the test set. In this case we use CountVectorizer.transform() instead of CountVectorizer.fit_transform() in order to mantain the vocabulary built before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42TE6_IowS5_"
      },
      "source": [
        "X_test_counts = vectorizer.transform(X_test)\n",
        "features_name = vectorizer.get_feature_names()\n",
        "features_name[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmpTvKnHwpbs"
      },
      "source": [
        "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9C5W8T-wpkF"
      },
      "source": [
        "prediction = regression.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aH60CLBwter"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBgXmV5jwwkG"
      },
      "source": [
        "np.mean(prediction == y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb5jDscafLIy"
      },
      "source": [
        "\n",
        "<a id='section2'></a>\n",
        "# **Explainability Methods**\n",
        "Explainability .... \n",
        "\n",
        "<a id='section2.1'></a>\n",
        "## **SHAP**\n",
        "SHAP ipsum lorem \n",
        "\n",
        "<a id='section2.2'></a>\n",
        "## **LIME**\n",
        "Lime ipsum lorem "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHLaBRQtfLN7"
      },
      "source": [
        ""
      ]
    }
  ]
}